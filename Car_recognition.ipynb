{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Recognition with Deep Learning\n",
    "\n",
    "This tutorial shows how Neural Networks are used to recognize cars on images vs. images with no cars on them (binary classification).\n",
    "\n",
    "The data set used is the [UIUC Image Database for Car Detection](http://cogcomp.cs.illinois.edu/Data/Car/) containing:\n",
    "* 1050 training images (550 car and 500 non-car images)\n",
    "* 170 test images, containing 200 cars at roughly the same scale as in the training images \n",
    "(we do not use the multi-scale test images, containing 139 cars at various scales, here)\n",
    "\n",
    "This tutorial contains:\n",
    "* Image Loading and Preprocessing\n",
    "* Standardization of Data\n",
    "* Fully Connected Neural Networks\n",
    "* Convolutional Neural Networks\n",
    "* Batch Normalization\n",
    "* ReLU Activation\n",
    "* Dropout\n",
    "* Data Augmentation\n",
    "\n",
    "You can execute the following code blocks by pressing SHIFT+Enter consecutively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is disabled, cuDNN 5105)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# if you want to use the GPU\n",
    "#device = 'gpu'\n",
    "#os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=' + device + ',floatX=float32'\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from theano import config\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Images from Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1050 files\n"
     ]
    }
   ],
   "source": [
    "path = 'data/CarData/TrainImages'\n",
    "files = glob.glob(os.path.join(path, '*.pgm'))\n",
    "print \"Found\", len(files), \"files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "image_names = []\n",
    "\n",
    "for filename in files:\n",
    "    image_names.append(os.path.basename(filename))\n",
    "    with Image.open(filename) as img:\n",
    "        images.append(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos-315.pgm\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAoCAAAAAAtEwCfAAAMNklEQVR4nIVXW4yc91X//S/f/Zv7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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x40 at 0x7F779596F0D0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=5\n",
    "print image_names[i]\n",
    "Image.fromarray(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[i].shape   # height x width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Color RGB images have an additional dimension of depth 3, e.g. (40, 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make 1 big array of list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 40, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list of many 40x100 images is made into 1 big array\n",
    "# config.floatX is from Theano configration to enforce float32 precision (needed for GPU computation)\n",
    "img_array = np.array(images, dtype=config.floatX)\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Groundtruth based on filenames:\n",
    "\n",
    "In this data set, images with cars start with \"pos-\" and images with no cars start with \"neg-\". We create a numeric list here containing 1 for car images and 0 for non-car images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = []\n",
    "for name in image_names:\n",
    "    if name.startswith('neg'):\n",
    "        classes.append(0)\n",
    "    else:\n",
    "        classes.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first 25 classes\n",
    "classes[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundtruth Statistics:\n",
      "Class 0 : 500\n",
      "Class 1 : 550\n"
     ]
    }
   ],
   "source": [
    "print \"Groundtruth Statistics:\"\n",
    "\n",
    "for v in set(classes):\n",
    "    print \"Class\", v, \":\", classes.count(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5238095238095238"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = 550 * 1.0 / len(classes)\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline is 52.3%, i.e. a 'dumb' classifier can assign all predictions to the majority class achieving this accurcay. We aim at building a classifier that performs better than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "<b>Always standardize</b> the data before feeding it into the Neural Network!\n",
    "\n",
    "Here we use <b>Zero-mean Unit-variance standardization</b> (flat, i.e. one mean and std.dev. for the whole image is computed over all pixels; in RGB images, standardization can be done e.g. for each colour channel individually; in other/non-image data sets, attribute-wise standardization should be applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.582 74.2767\n"
     ]
    }
   ],
   "source": [
    "mean = img_array.mean()\n",
    "stddev = img_array.std()\n",
    "print mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.27098e-07 1.0\n"
     ]
    }
   ],
   "source": [
    "img_array = (img_array - mean) / stddev\n",
    "print img_array.mean(), img_array.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.7445904, 1.6885173)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.min(), img_array.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating NN Models in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1) Fully Connected NN\n",
    "\n",
    "For a fully connected neural network, the x and y axis of an image do not play a role at all. All pixels are considered as a completely individual input to the neural network. Therefore the 2D image arrays have to be flattened to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 4000)\n"
     ]
    }
   ],
   "source": [
    "#  flatten images to vectors\n",
    "images_flat = img_array.reshape(img_array.shape[0],-1)\n",
    "print images_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "# find out input shape for NN, which is just a long vector (40x100 = 4000)\n",
    "input_shape = images_flat.shape[1]\n",
    "print input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model\n",
    "\n",
    "In Keras, one can choose between a Sequential model and a Graph model. Sequential models are the standard case. Graph models are for parallel networks.\n",
    "\n",
    "Here we create a sequential model with 2 fully connected (a.k.a. 'dense') layers containing 256 units each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simple Fully-connected network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_dim=input_shape))\n",
    "\n",
    "model.add(Dense(256))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 256)           1024256     dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 256)           65792       dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             257         dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1090305\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss Function and Optimizer Strategy: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'binary_crossentropy'  # 'categorical_crossentropy' for multi-class problems\n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model\n",
    "# This creates the whole model structure in memory. \n",
    "# If you use GPU computation, here GPU compatible structures and code is generated.\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.5297 - acc: 0.7819     \n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.3020 - acc: 0.8629     \n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.2333 - acc: 0.8962     \n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1989 - acc: 0.9229     \n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1526 - acc: 0.9467     \n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1229 - acc: 0.9638     \n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0995 - acc: 0.9790     \n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0823 - acc: 0.9838     \n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0741 - acc: 0.9867     \n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0622 - acc: 0.9895     \n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0497 - acc: 0.9971     \n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0451 - acc: 0.9933     \n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0357 - acc: 0.9990     \n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0323 - acc: 0.9990     \n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0272 - acc: 0.9990     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77989b4d50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "model.fit(images_flat, classes, batch_size=32, nb_epoch=epochs) #, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 992/1050 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify Accuracy on Train set\n",
    "predictions = model.predict_classes(images_flat)\n",
    "accuracy_score(classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% Accuracy - perfect, no?\n",
    "\n",
    "This is the accuracy on the training set. A (large, especially fully connected network with sufficient number of units) can easily learn the entire training set (especially a small one like here).\n",
    "\n",
    "This very likely leads to <b>overfitting</b>. That's why we test on an independent test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 170 files\n"
     ]
    }
   ],
   "source": [
    "path = 'data/CarData/TestImages'\n",
    "files = glob.glob(os.path.join(path, '*.pgm'))\n",
    "print \"Found\", len(files), \"files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from image_preprocessing import resize_and_crop\n",
    "\n",
    "test_images = []\n",
    "\n",
    "for filename in files:\n",
    "    with Image.open(filename) as img:\n",
    "        img_resized = resize_and_crop(img,target_width=100,target_height=40)\n",
    "        test_images.append(np.array(img_resized))\n",
    "        #print img.size, img_resized.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAoCAAAAAAtEwCfAAAMG0lEQVR4nDXUyY+l51WA8XPO+77f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\nHye3L2cfHM0uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x40 at 0x7F775C00ABD0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=3\n",
    "Image.fromarray(test_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make 1 big array again from list\n",
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Test Set\n",
    "\n",
    "The test data has to be standardized <b>in the same way</b> as the training data for compatibility with the model! That means, we take the mean and standard deviation of the <i>training data</i> to transform also the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NO! we take the same mean and stddev from the training data above!\n",
    "#mean = test_images.mean()\n",
    "#stddev = test_images.std()\n",
    "#print mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121.71342647058823, 72.05232875177191)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.mean(), test_images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_images = (test_images - mean) / stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Images for Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 4000)\n"
     ]
    }
   ],
   "source": [
    "test_images_flat = test_images.reshape(test_images.shape[0],-1)\n",
    "print test_images_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/170 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_images_flat)\n",
    "# show 30 first predictions\n",
    "test_pred[0:30,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Groundtruth:\n",
    "# this TEST SET contains ONLY CARS on images! \n",
    "# Thus all the test classes are 1\n",
    "test_classes = [1] * len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's count the number of ones ...\n",
    "test_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4823529411764706"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(test_classes, test_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117647058823529"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "70.0/170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the Test Set is only 41.17%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which groups neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Our input to the CNN is the standardized version of the original image array.\n",
    "\n",
    "#### Adding the channel\n",
    "\n",
    "For CNNs, we need to add a dimension for the color channel to the data. RGB images typically have an 3rd dimension with the color. \n",
    "<b>For greyscale images we need to add an extra dimension for compatibility with the CNN implementation.</b>\n",
    "\n",
    "In Theano, traditionally the color channel was the <b>first</b> dimension in the image shape. \n",
    "In Tensorflow, the color channel is the <b>last</b> dimension in the image shape. \n",
    "\n",
    "This can be configured now in ~/.keras/keras.json: \"image_dim_ordering\": \"th\" or \"tf\" with \"tf\" (Tensorflow) being the default image ordering even though you use Theano. Depending on this, use one of the code lines below.\n",
    "\n",
    "For greyscale images, we add the number 1 as the depth of the additional dimension of the input shape (for RGB color images, the number of channels is 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_channels = 1 # for grey-scale, 3 for RGB, but usually already present in the data\n",
    "\n",
    "if keras.backend.image_dim_ordering() == 'th':\n",
    "    # Theano ordering (~/.keras/keras.json: \"image_dim_ordering\": \"th\")\n",
    "    train_img = img_array.reshape(img_array.shape[0], n_channels, img_array.shape[1], img_array.shape[2])\n",
    "    test_img = test_images.reshape(test_images.shape[0], n_channels, test_images.shape[1], test_images.shape[2])\n",
    "else:\n",
    "    # Tensorflow ordering (~/.keras/keras.json: \"image_dim_ordering\": \"tf\")\n",
    "    train_img = img_array.reshape(img_array.shape[0], img_array.shape[1], img_array.shape[2], n_channels)\n",
    "    test_img = test_images.reshape(test_images.shape[0], test_images.shape[1], test_images.shape[2], n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 40, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 1, 40, 100)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of images)\n",
    "    \n",
    "input_shape = train_img.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "n_filters = 16\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "\n",
    "model.add(Convolution2D(n_filters, 5, 5, border_mode='valid', input_shape=input_shape))\n",
    "# input shape: 100x100 images with 3 channels -> input_shape should be (3, 100, 100) \n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.3)) \n",
    "\n",
    "# B)\n",
    "model.add(Convolution2D(n_filters, 3, 3))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten()) # Note: Keras does automatic shape inference.\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get OverflowError: Range exceeds valid bounds in the above box, check the correct Theano vs. Tensorflow ordering in the box before and your keras.json configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 16, 36, 96)    416         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 16, 36, 96)    192         convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 16, 36, 96)    0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 16, 18, 48)    0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 16, 18, 48)    0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 16, 16, 46)    2320        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 16, 16, 46)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 16, 16, 46)    0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 11776)         0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 256)           3014912     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 256)           0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 256)           0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             257         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3018097\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "loss = 'binary_crossentropy' \n",
    "optimizer = 'sgd' \n",
    "#optimizer = SGD(lr=0.001)  # possibility to adapt the learn rate\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Validation Data\n",
    "\n",
    "We split off 10 % of the training data and use it as independend validation set to verify how good we are\n",
    "on an independent data (not used for training) in each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945 samples, validate on 105 samples\n",
      "Epoch 1/15\n",
      "945/945 [==============================] - 5s - loss: 0.0698 - acc: 0.9788 - val_loss: 0.0683 - val_acc: 0.9810\n",
      "Epoch 2/15\n",
      "945/945 [==============================] - 5s - loss: 0.0489 - acc: 0.9841 - val_loss: 0.0451 - val_acc: 0.9905\n",
      "Epoch 3/15\n",
      "945/945 [==============================] - 5s - loss: 0.0314 - acc: 0.9905 - val_loss: 0.0319 - val_acc: 0.9905\n",
      "Epoch 4/15\n",
      "945/945 [==============================] - 5s - loss: 0.0284 - acc: 0.9905 - val_loss: 0.0340 - val_acc: 0.9905\n",
      "Epoch 5/15\n",
      "945/945 [==============================] - 5s - loss: 0.0227 - acc: 0.9947 - val_loss: 0.0282 - val_acc: 0.9905\n",
      "Epoch 6/15\n",
      "945/945 [==============================] - 5s - loss: 0.0173 - acc: 0.9947 - val_loss: 0.0324 - val_acc: 0.9905\n",
      "Epoch 7/15\n",
      "945/945 [==============================] - 5s - loss: 0.0107 - acc: 0.9989 - val_loss: 0.0236 - val_acc: 0.9905\n",
      "Epoch 8/15\n",
      "945/945 [==============================] - 5s - loss: 0.0083 - acc: 0.9989 - val_loss: 0.0201 - val_acc: 0.9905\n",
      "Epoch 9/15\n",
      "945/945 [==============================] - 5s - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0201 - val_acc: 0.9905\n",
      "Epoch 10/15\n",
      "945/945 [==============================] - 5s - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0194 - val_acc: 0.9905\n",
      "Epoch 11/15\n",
      "945/945 [==============================] - 5s - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0218 - val_acc: 0.9905\n",
      "Epoch 12/15\n",
      "945/945 [==============================] - 5s - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0205 - val_acc: 0.9905\n",
      "Epoch 13/15\n",
      "945/945 [==============================] - 5s - loss: 0.0055 - acc: 0.9989 - val_loss: 0.0212 - val_acc: 0.9905\n",
      "Epoch 14/15\n",
      "945/945 [==============================] - 5s - loss: 0.0058 - acc: 0.9989 - val_loss: 0.0173 - val_acc: 0.9905\n",
      "Epoch 15/15\n",
      "945/945 [==============================] - 5s - loss: 0.0055 - acc: 0.9989 - val_loss: 0.0332 - val_acc: 0.9810\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs, validation_split=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the split-off validation data are quite high (usually similar, but not as high as on the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Test Set as Validation Set\n",
    "\n",
    "<b>Note: This usually is not recommended as during experimentation you will overfit also to the test data.</b>\n",
    "\n",
    "We show it here only for demonstration purposes to see how (bad) the validation accuracy is on our independet test data. The recommended way is to have a separate training, validation and test set (i.e. 3 splits or separate data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1050 samples, validate on 170 samples\n",
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0058 - acc: 0.9981 - val_loss: 6.4739 - val_acc: 0.1118\n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0071 - acc: 0.9981 - val_loss: 6.2653 - val_acc: 0.1176\n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0044 - acc: 1.0000 - val_loss: 6.3239 - val_acc: 0.1176\n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0040 - acc: 1.0000 - val_loss: 7.3631 - val_acc: 0.0941\n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0037 - acc: 1.0000 - val_loss: 6.8158 - val_acc: 0.1118\n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0047 - acc: 0.9990 - val_loss: 7.2102 - val_acc: 0.1059\n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0039 - acc: 0.9990 - val_loss: 7.0418 - val_acc: 0.1176\n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0036 - acc: 1.0000 - val_loss: 7.3534 - val_acc: 0.1118\n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0070 - acc: 0.9971 - val_loss: 6.8198 - val_acc: 0.1118\n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0041 - acc: 0.9981 - val_loss: 7.0478 - val_acc: 0.1118\n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0039 - acc: 0.9990 - val_loss: 6.9055 - val_acc: 0.1176\n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0018 - acc: 1.0000 - val_loss: 7.0813 - val_acc: 0.1118\n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0035 - acc: 0.9990 - val_loss: 7.4954 - val_acc: 0.1059\n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0014 - acc: 1.0000 - val_loss: 7.6377 - val_acc: 0.1059\n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0016 - acc: 1.0000 - val_loss: 7.2672 - val_acc: 0.1176\n"
     ]
    }
   ],
   "source": [
    "# Show Result on Test Data while training \n",
    "# we use test data as validation data to see direct results (usually NOT RECOMMENDED due to overfitting to the problem!)\n",
    "validation_data = (test_img, test_classes)\n",
    "\n",
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_img)\n",
    "# show 35 first predictions\n",
    "test_pred[0:35,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.58823529411764708"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(test_img)\n",
    "accuracy_score(test_classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: We are completely overfitting!\n",
    "While we achieve nearly 100% on the Training Set, the generalization to the Test Set is really bad, with an Accuracy of about only 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Augmentation\n",
    "\n",
    "Increase the training set by adding more images: Rotate, shift, flip and scale the original images to generate additional examples that will help the Neural Network to generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    #zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "#datagen.fit(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ImageDataGenerator needs the classes as Numpy array instead of normal list\n",
    "classes_array = np.array(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1626 - acc: 0.9400 - val_loss: 1.4034 - val_acc: 0.4118\n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1518 - acc: 0.9352 - val_loss: 1.2689 - val_acc: 0.4588\n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1627 - acc: 0.9333 - val_loss: 1.4350 - val_acc: 0.4059\n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1566 - acc: 0.9438 - val_loss: 1.1026 - val_acc: 0.5000\n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1669 - acc: 0.9343 - val_loss: 1.1472 - val_acc: 0.4824\n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1508 - acc: 0.9419 - val_loss: 1.0082 - val_acc: 0.5235\n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1944 - acc: 0.9248 - val_loss: 1.1814 - val_acc: 0.4294\n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1447 - acc: 0.9400 - val_loss: 1.4667 - val_acc: 0.4059\n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1661 - acc: 0.9371 - val_loss: 1.5967 - val_acc: 0.3647\n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1636 - acc: 0.9314 - val_loss: 0.9733 - val_acc: 0.5529\n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1701 - acc: 0.9362 - val_loss: 1.0711 - val_acc: 0.5059\n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1604 - acc: 0.9410 - val_loss: 0.8782 - val_acc: 0.5824\n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1574 - acc: 0.9448 - val_loss: 1.7521 - val_acc: 0.3471\n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1482 - acc: 0.9448 - val_loss: 1.4979 - val_acc: 0.3706\n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1504 - acc: 0.9410 - val_loss: 1.5066 - val_acc: 0.3765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History object at 0x7f3b5a61f390>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(train_img, classes_array, batch_size=32),\n",
    "                    samples_per_epoch=len(train_img), nb_epoch=epochs,\n",
    "                    validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set (with Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_img)\n",
    "# show 35 first predictions\n",
    "test_pred[0:35,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44117647058823528"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(test_img)\n",
    "accuracy_score(test_classes, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
