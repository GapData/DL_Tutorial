{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Recognition with Deep Learning\n",
    "\n",
    "This tutorial shows how Deep Neural Networks are used to recognize cars on images versus images with no cars on them (binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=' + device + ',floatX=float32,nvcc.flags=-D_FORCE_INLINES'\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from theano import config\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "#from keras.optimizers import SGD, RMSprop, Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed?\n",
    "import keras\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import pandas as pd # Pandas for easier Data handling in preparation\n",
    "\n",
    "from theano import function as tfunction\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import Callback, History, EarlyStopping, ModelCheckpoint # BaseLogger,\n",
    "\n",
    "import json\n",
    "#import cPickle # for saving scaler and labelencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Images from Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1050 files\n"
     ]
    }
   ],
   "source": [
    "path = 'data/CarData/TrainImages'\n",
    "files = glob.glob(os.path.join(path, '*.pgm'))\n",
    "print \"Found\", len(files), \"files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "image_names = []\n",
    "\n",
    "for filename in files:\n",
    "    image_names.append(os.path.basename(filename))\n",
    "    with Image.open(filename) as img:\n",
    "        images.append(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos-315.pgm\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAoCAAAAAAtEwCfAAAMNklEQVR4nIVXW4yc91X//S/f/Zv7\nzM7srr32ru34ltqOc2muiFY0jQi0AVoqofKC1MA78IKQAAmeeCkgtUhIlagQCgqgEJrQQkkb13WT\nrI2xnV1nfVnv2rs7OzM7szPffPfvf+HBTcML4jwc6ehI5+ic3zk6v0POaGWVhNLF3NOrwdKZY/R7\nbxuvPO9NEVy7vJVkYZ55vudaitF4DCpzk0lXZlEhTKUAJAyAzBUg2l95QW4dOLrxD4/+Ev/WXwBg\nBLYlwlnwSRodak+LJN+6MDuza+yIH105e7YxtVn/h9d9Cp+OU8MgalA+4IipI3arpWIUTs0FPhkA\nABgHuDWVQO2k7N3ibeLfeLHjAIDkJoQkDnjlQJnGSgEG1XkcLi8vzOpLpdn87ddKxx6YgKoalmvb\nzurwqYPv1B17LLO8gANNDQAAf6gVYHhyMk4jpccFI2AAAKVAQ8XdpZOjD7TWCkwkwzx0OBh33av/\nuPwUBQDluH6pZD2RRJ9pbq3UXUylQsmVkroCABQAABSQu6Vxwa1MSTke4mfChzmXk15MfmqnMsRc\n0D/38tLNv7p2JkhsBUCJzGTp+fPBI87Ba4khJCiQgxaMAUCmAMChSNTa4f0clMDEZPeTJFDgQfrA\n9gWBgiEyJIcXYvzXn3w5CPKpLyMHlA5ZJTGNfzcO39qJGtOYMg5wNcisFoeiVMYcAq4JS95n41wx\nKpJETT7JISoZ343E7KEsK7hDlZJG1nXmu1fvd45Z+35ntKOitCK2mF9699S2ZEGOacsaK89IjCP5\n2DC4iAUoNcEoICYDQak2GSJIgAAUAFAFh8cny+xgqxVed4CizIOgF88cOvG5tQtynjGvtGDcT3jd\n9G52fuPSX4ZuizRr3ftxpRGsNTpqyFTBLcccJVXflJPmST8qf+7erixXAIBzADRI+F+DdS+brU7d\nyAEgzxnjdGlpqs48vRm52dkjNWIncBgKQhpvKjMjzUY3g7wzetwYOLUp2qZhGOHEpxinlcW5Emvt\n/ctOJAEo01cUis/wReDQ05px5tUBAIQwQGQdXVuoXb1/4z/pZx9bmNu7kKk38HuNvSG6DzsdhrjC\nG6JsiRSEkqrHmN7jS7U59CuPvLsKX4FSBaUgOXgLoGYOThE8nEXOKRvsjUfjYmVtr2NM//TJ12Q6\nDc0XzQONvxkoGUhoVGj3Ut/0hnFvoplTYjOeprLTOFkzhdJiCoQAEE40JJMgAxBoodTHMCmloNLp\njqg3ML7Wnj3hmwQgRGug4IT8DE9VDK+k9rgwFagRCZPGk1q7ZouBR9UwEUISHe9/JIimmgyQKziA\nAFEKuWmKtMjqbhBoPWNnviUHN+69v1cAedHuccZF/qlXF3tXxl+YJVCX//z4C/VZl2IkPahMZlEQ\ncUoX5yRTOTjSHKBZTH4nW7BMw7crhnIJXIKCctegUEVUbWF3+dvd8tJc3Vm5U5Qf/bm6L9e/e++X\nj402J0cXzpeBYvDNi4uvnOLaUkXJRtzbipspO9QWrEgLZlNhcCZi8nJmgVm+d8zSe4I+ZStBTYtR\n0yoZ0Si9tTwzU0SJ07q5mpcfe9at1rs9OedN96NGkVVnWiYplr9Re7xxrDxKuTfYM1zCxn7NLLJy\nneYwU84YAbkrg0K5FAYw1bAICEHC7SpN3x+esEwepUSqrHd7Na93uHnw+PZe/Ui56G2mgTfbrDXa\nHOrO97/7lTO97v1+tvjIAk0zceFiFvzWr8zGkWcrqZXJy5LkMhfKNVGhCDUcqHmVRtPYOD7vRKM+\naVaqk10A2ixtJ8mUlwPHrYU7TtnSkcxnLP7I0S+sLk9SjLVVo2F4MBdbW/B1PE2lgFIx5yKvkWIq\nRF6gACEajCpDJkGSLxpxnmuHUZ1JgwOyeaY6UrMtq2TYVepSMVg3l4xdVsz7i3Pr23u53TpQURGK\n4sDhRodu9kcxVwb3shbZK6guhJAWCqFlwQ1PxbuuX+fQyc6Hav6IW4QqvLJ5V/mnFt4fHJ9t5yjX\nunv7ljW+cfrJTNYbKT9YZUQXIh5Od2SbYry2XQ0cvrGnTmLC4ufIvtLFNAnDssnAqGra8s7ff+/P\nXqIrb344Za7AoYp5dP7GH9q2L/yOErnm55qJN8Jx8IZtI1C+k6cOrzZtTokcvj44J6Pr/x2VX8C9\n4X76GEZZtEA2c1DkcXqWUxAkUtz79hsHvvzoXnx7xqKlaVot05ly9pHmlqrP6zwBO2EqxZkmKNTm\nTeGXhMGUT2PZu+w/WxmNosP08hXn/PRq32RBg4rIsEkAEwCoEHHSH9m+GzzY+OdPf97/lEEpFFAI\nyhQIAE2IZhRKSVls92/fLGaeY3omCTmnmlsiGN0rzdZcdFVJ7WxEcmXULsez++PYL5OU5qnoT6aE\nKi2EybgKhsOLg7LlLXl0RkUP9ptnXbHOoh3qNBSzoXLbLTlUzdPAjBJQAwaHYiggK1SbLAnibHtt\nVAkSRVWWZbAtsjJGOlRZel86jsXXEj5nFQpWDek4poXh1uX69rj0sppucABKaf+gRynRo/cqZzlV\nsBk4RyxN5vAhLTkW2M5ba5Vzt1jSTWGwmBqMvDaWQMnAPe1Q24qnk6jd7NW9/TwDkREKnaRm4p3Q\nIgdzaB6BW6KIYtREs65MWwgKg5BYetSzp6pK18RptbstnR8Uk3V4S7xHbYP8ehrQCiuVVF6ULbAi\nzAwWWTyRgAKzAEIJYxKUwGKAFp5QVJl1s5yOU3ueJZFirgFUUs8iW0Z926klDrYehMGtHeq0yZg5\nnDwtVmMwy3jWk0TDAAFgaRCpATDz4YkyOACYHIDwACirlu2mFbtki31hmZzZNI+i6dzRyjQrX/9g\nN2aV83O0f3XL9byyY/B7qePQRBW7tlUyjIeUDB9zpP9LqI52rmUkIMwgzfNuLhTLFaHNp8gfXVly\ny4V6/EmH0qpiw1QV3OBpMm+micBWucQNS+r/JzxMqgCa3usPkgab0hjRfuN051Q5nEyn18tO7Rfp\npWO/ltukNlm9bh7KkyizOK/iqBNWh8lgMu+WDSHBAFAUHxMz0P9dmYJgAGXRR5KUGqpUDvu9YLD/\nmZ0+pCjduuA/VV51t2+cs/OVoF+Za3mouAbnpnzpeG8nwnfWY21UmEasiU50fZoDMOtE5RJCaADg\noAZVnpYSwb2yXZtbaM3R1Tc/VO79Hevxo3X3VNcqvkVfWhk2T3pfE881RLbEickoeX72GWwlDk9v\nrs80a8dUGineuNj7/c0NE+A8L5KcJJQBQKdMLZeZSgTh5o+bc+c/bdxdXzoh3ngnW2w3GpzRtu8W\nYx3ctZ+teT+ZyqEgTxgTbhj86Fclfybvbm0IcY02ulQd5tRapH9X8zMg7CrbsIuyzQGAJJZXCd9i\nvswzz5lN/vX5I500FmfD9zaHC3Nq13RVSIAIyao5O40EKF22mxbnfPuPZ195tBYNe0mYWciZ3qXq\nQuVAOJkAYAvZcFCQvH1CAhiGIs/dz8aTYR6uNcXpA6J8Ynz1dcGNYseZlByHQwDwDhbSiH+w71UZ\n7hiLDc/iX/M7Lbv7zn9U06lVkoNMtAgqcsNhAJQx5DMnPeHdXqEA9jOrsaTfc+plzzzVOne8UTOV\n/diJ/vc3MkS3T1ZsohRAC8ltwy1t360zzo1MAvzfvjgJHPbK89+8lecxXI8PYNf716VZUaBRjPaC\nI93hR5QBTbrmbReb8zWTFzcacceeznu3L9UOdQA37dUWZS4EpUoAOYIvuZFQKDkoopBfX3u5k/oz\ntUn/yFedS2/HrD7JozO//XQiQDSRk4/e2jHv+IchU2rQz7/YqFnJrc29u4+7nWjvWLZW6A/WbueO\nlYx6tX6lyrKJW1ey1F6f/LwvUzVNhdolZBBQPioqN76x+AfzdPs3LwCPdl4vEYpPVkMDeKgI9EOH\nVttvfudLXxyz5t7Ftx90u0742K96bxT2q6Ovn39p9cbcE7n93IF83a9s359pmdxsaNHWzi+8WOQ5\nr339n9acZ59HLC0OQX+6iw8/Ng5AAhKgACGllXfHl1+tbvz4b5cB5Ki22r/7k4t3vONPuM1KOpls\nB3p4otOPh5PrPTKI8yqHUpzoMDd9Q+gwUmlerZljihDEgsoBwgkFVM68MDIciIadrGwo+5H6Xlda\nDtGKloNleXPz9OkQM5Md7h/09TM1UCV4sT7+H4zxjCDt0TT+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x40 at 0x7FD7897EAD10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=5\n",
    "print image_names[i]\n",
    "Image.fromarray(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[i].shape   # height x width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Color RGB images have an additional dimension of depth 3, e.g. (40, 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make 1 big array of list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 40, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list of many 40x100 images is made into 1 big array\n",
    "# config.floatX is from Theano configration to enforce float32 precision (needed for GPU computation)\n",
    "img_array = np.array(images, dtype=config.floatX)\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Groundtruth based on filenames:\n",
    "\n",
    "In this data set, images with cars start with \"pos-\" and images with no cars start with \"neg-\". We create a numeric list here containing 1 for car images and 0 for non-car images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = []\n",
    "for name in image_names:\n",
    "    if name.startswith('neg'):\n",
    "        classes.append(0)\n",
    "    else:\n",
    "        classes.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first 20 classes\n",
    "classes[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundtruth Statistics:\n",
      "Class 0 : 500\n",
      "Class 1 : 550\n"
     ]
    }
   ],
   "source": [
    "print \"Groundtruth Statistics:\"\n",
    "\n",
    "for v in set(classes):\n",
    "    print \"Class\", v, \":\", classes.count(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5238095238095238"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = 550 * 1.0 / len(classes)\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "<b>Always standardize</b> the data before feeding it into the Neural Network!\n",
    "\n",
    "Here we use <b>Zero-mean Unit-variance standardization</b> (flat, i.e. one mean and std.dev. for the whole image is computed over all pixels; in RGB images, standardization can be done e.g. for each colour channel individually; in other/non-image data sets, attribute-wise standardization should be applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.582 74.2767\n"
     ]
    }
   ],
   "source": [
    "mean = img_array.mean()\n",
    "stddev = img_array.std()\n",
    "print mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.27098e-07 1.0\n"
     ]
    }
   ],
   "source": [
    "img_array = (img_array - mean) / stddev\n",
    "print img_array.mean(), img_array.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.7445904, 1.6885173)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.min(), img_array.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating NN Models in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1) Fully Connected NN\n",
    "\n",
    "For a fully connected neural network, the x and y axis of an image do not play a role at all. All pixels are considered as a completely individual input to the neural network. Therefore the 2D image arrays have to be flattened to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 4000)\n"
     ]
    }
   ],
   "source": [
    "#  flatten images to vectors\n",
    "images_flat = img_array.reshape(img_array.shape[0],-1)\n",
    "print images_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "# find out input shape for NN, which is just a long vector (40x100 = 4000)\n",
    "input_shape = images_flat.shape[1]\n",
    "print input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model\n",
    "\n",
    "In Keras, one can choose between a Sequential model and a Graph model. Sequential models are the standard case. Graph models are for parallel networks.\n",
    "\n",
    "Here we create a sequential model with 2 fully connected (a.k.a. 'dense') layers containing 256 units each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simple Fully-connected network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_dim=input_shape))\n",
    "\n",
    "model.add(Dense(256))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 256)           1024256     dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 256)           65792       dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             257         dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1090305\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "# This creates the whole model structure in memory. \n",
    "# If you use GPU computation, here GPU compatible structures and code is generated.\n",
    "\n",
    "loss = 'binary_crossentropy' \n",
    "optimizer = 'sgd' \n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.5338 - acc: 0.7724     \n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.3091 - acc: 0.8419     \n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.2274 - acc: 0.8933     \n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1849 - acc: 0.9229     \n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1580 - acc: 0.9448     \n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1265 - acc: 0.9638     \n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1055 - acc: 0.9695     \n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0841 - acc: 0.9857     \n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0801 - acc: 0.9771     \n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0602 - acc: 0.9905     \n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0491 - acc: 0.9952     \n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0450 - acc: 0.9943     \n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0383 - acc: 0.9962     \n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0319 - acc: 0.9990     \n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0276 - acc: 1.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd74fb8b910>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "model.fit(images_flat, classes, batch_size=32, nb_epoch=epochs) #, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 928/1050 [=========================>....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify Accuracy on Train set\n",
    "predictions = model.predict_classes(images_flat)\n",
    "accuracy_score(predictions, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% Accuracy - perfect, no?\n",
    "\n",
    "This is the accuracy on the training set. A (large, especially fully connected network with sufficient number of units) can easily learn the entire training set (especially a small one like here).\n",
    "\n",
    "This very likely leads to <b>overfitting</b>. That's why we test on an independent test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 170 files\n"
     ]
    }
   ],
   "source": [
    "path = 'data/CarData/TestImages'\n",
    "files = glob.glob(os.path.join(path, '*.pgm'))\n",
    "print \"Found\", len(files), \"files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from image_preprocessing import resize_and_crop\n",
    "\n",
    "test_images = []\n",
    "\n",
    "for filename in files:\n",
    "    with Image.open(filename) as img:\n",
    "        img_resized = resize_and_crop(img,target_width=100,target_height=40)\n",
    "        test_images.append(np.array(img_resized))\n",
    "        #print img.size, img_resized.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAoCAAAAAAtEwCfAAAMG0lEQVR4nDXUyY+l51WA8XPO+77f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\nHye3L2cfHM0uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x40 at 0x7FD754656F50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=3\n",
    "Image.fromarray(test_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make 1 big array again from list\n",
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Test Set\n",
    "\n",
    "The test data has to be standardized <b>in the same way</b> as the training data for compatibility with the model! That means, we take the mean and standard deviation of the <i>training data</i> to transform also the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NO! we take the same mean and stddev from the training data above!\n",
    "#mean = test_images.mean()\n",
    "#stddev = test_images.std()\n",
    "#print mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121.71342647058823, 72.05232875177191)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.mean(), test_images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_images = (test_images - mean) / stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Images for Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 4000)\n"
     ]
    }
   ],
   "source": [
    "test_images_flat = test_images.reshape(test_images.shape[0],-1)\n",
    "print test_images_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/170 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_images_flat)\n",
    "# show 30 first predictions\n",
    "test_pred[0:30,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Groundtruth:\n",
    "# this TEST SET contains ONLY CARS on images! \n",
    "# Thus all the test classes are 1\n",
    "test_classes = [1] * len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's count the number of ones ...\n",
    "test_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41176470588235292"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(test_classes, test_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117647058823529"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "70.0/170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the Test Set is only 41.17%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which groups neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Our input to the CNN is the standardized version of the original image array.\n",
    "\n",
    "#### Adding the channel\n",
    "\n",
    "For CNNs, we need to add a dimension for the color channel to the data. RGB images typically have an 3rd dimension with the color. \n",
    "<b>For greyscale images we need to add an extra dimension for compatibility with the CNN implementation.</b>\n",
    "\n",
    "In Theano, traditionally the color channel was the <b>first</b> dimension in the image shape. \n",
    "In Tensorflow, the color channel is the <b>last</b> dimension in the image shape. \n",
    "\n",
    "This can be configured now in ~/.keras/keras.json: \"image_dim_ordering\": \"th\" or \"tf\" with \"tf\" (Tensorflow) being the default image ordering even though you use Theano. Depending on this, use one of the code lines below.\n",
    "\n",
    "For greyscale images, we add the number 1 as the depth of the additional dimension of the input shape (for RGB color images, the number of channels is 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_channels = 1 # for grey-scale, 3 for RGB, but usually already present in the data\n",
    "\n",
    "if keras.backend.image_dim_ordering() == 'th':\n",
    "    # Theano ordering (~/.keras/keras.json: \"image_dim_ordering\": \"th\")\n",
    "    train_img = img_array.reshape(img_array.shape[0], n_channels, img_array.shape[1], img_array.shape[2])\n",
    "    test_img = test_images.reshape(test_images.shape[0], n_channels, test_images.shape[1], test_images.shape[2])\n",
    "else:\n",
    "    # Tensorflow ordering (~/.keras/keras.json: \"image_dim_ordering\": \"tf\")\n",
    "    train_img = img_array.reshape(img_array.shape[0], img_array.shape[1], img_array.shape[2], n_channels)\n",
    "    test_img = test_images.reshape(test_images.shape[0], test_images.shape[1], test_images.shape[2], n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40, 100)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of images)\n",
    "    \n",
    "input_shape = train_img.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "n_filters = 16\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "\n",
    "model.add(Convolution2D(n_filters, 5, 5, border_mode='valid', input_shape=input_shape))\n",
    "# input shape: 100x100 images with 3 channels -> input_shape should be (3, 100, 100) \n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.3)) \n",
    "\n",
    "# B)\n",
    "model.add(Convolution2D(n_filters, 3, 3))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten()) # Note: Keras does automatic shape inference.\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get OverflowError: Range exceeds valid bounds in the above box, check the correct Theano vs. Tensorflow ordering in the box before and your keras.json configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_2 (Convolution2D)  (None, 16, 36, 96)    416         convolution2d_input_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 16, 36, 96)    192         convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 16, 36, 96)    0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 16, 18, 48)    0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 16, 18, 48)    0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 16, 16, 46)    2320        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 16, 16, 46)    0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 16, 16, 46)    0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 11776)         0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 256)           3014912     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 256)           0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 256)           0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             257         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3018097\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "loss = 'binary_crossentropy' \n",
    "optimizer = 'sgd' #rmsprop'\n",
    "#optimizer = SGD(lr=0.001)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# DEBUG\n",
    "# we use test data as validation data to see direct results (usually not recommended)\n",
    "validation_data = (test_img, test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs, validation_split=0.1) #validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict_classes(test_img)\n",
    "# show 35 first predictions\n",
    "test_pred[0:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
