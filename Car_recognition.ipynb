{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Recognition with Deep Learning\n",
    "\n",
    "This tutorial shows how Neural Networks are used to recognize cars on images vs. images with no cars on them (binary classification).\n",
    "\n",
    "The data set used is the [UIUC Image Database for Car Detection](http://cogcomp.cs.illinois.edu/Data/Car/) containing:\n",
    "* 1050 training images (550 car and 500 non-car images)\n",
    "* 170 test images, containing 200 cars at roughly the same scale as in the training images \n",
    "(we do not use the multi-scale test images, containing 139 cars at various scales, here)\n",
    "\n",
    "This tutorial contains:\n",
    "* Image Loading and Preprocessing\n",
    "* Standardization of Data\n",
    "* Fully Connected Neural Networks\n",
    "* Convolutional Neural Networks\n",
    "* Batch Normalization\n",
    "* ReLU Activation\n",
    "* Dropout\n",
    "* Data Augmentation\n",
    "\n",
    "You can execute the following code blocks by pressing SHIFT+Enter consecutively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# if you want to use the GPU\n",
    "#device = 'gpu'\n",
    "#os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=' + device + ',floatX=float32'\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from theano import config\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "np.random.seed(1) # we initialize a random seed here to make the experiments repeatable with same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Images from Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1050 files\n"
     ]
    }
   ],
   "source": [
    "path = 'data/CarData/TrainImages'\n",
    "files = glob.glob(os.path.join(path, '*.pgm'))\n",
    "print \"Found\", len(files), \"files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "image_names = []\n",
    "\n",
    "for filename in files:\n",
    "    image_names.append(os.path.basename(filename))\n",
    "    with Image.open(filename) as img:\n",
    "        images.append(np.array(img)) # we convert the images to a Numpy array and store them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos-207.pgm\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAoCAAAAAAtEwCfAAAKC0lEQVR4nEWXQYgc17WGP0HDPRcy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\nTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x40 at 0x7F8B6EA83790>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show a selected image to check\n",
    "i=1\n",
    "print image_names[i]\n",
    "Image.fromarray(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[i].shape   # height x width   (Numpy ordering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Color RGB images have an additional dimension of depth 3, e.g. (40, 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make 1 big array of list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 40, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list of many 40x100 images is made into 1 big array\n",
    "# config.floatX is from Theano configration to enforce float32 precision (needed for GPU computation)\n",
    "img_array = np.array(images, dtype=config.floatX)\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Groundtruth based on filenames:\n",
    "\n",
    "In this data set, images with cars start with \"pos-\" and images with no cars start with \"neg-\". We create a numeric list here, containing 1 for car images and 0 for non-car images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = []\n",
    "for name in image_names:\n",
    "    if name.startswith('neg'):\n",
    "        classes.append(0)\n",
    "    else:\n",
    "        classes.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first 25 classes\n",
    "classes[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundtruth Statistics:\n",
      "Class 0 : 500\n",
      "Class 1 : 550\n"
     ]
    }
   ],
   "source": [
    "print \"Groundtruth Statistics:\"\n",
    "\n",
    "for v in set(classes):\n",
    "    print \"Class\", v, \":\", classes.count(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5238095238095238"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = 550 * 1.0 / len(classes)\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline is 52.3%, i.e. a 'dumb' classifier can assign all predictions to the majority class achieving this accurcay. We aim at building a classifier that performs better than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "<b>Always standardize</b> the data before feeding it into the Neural Network!\n",
    "\n",
    "Here we use <b>Zero-mean Unit-variance standardization</b> which means we deduct the mean and divide by the standard deviation.\n",
    "\n",
    "(Note: Here, we do this \"flat\", i.e. one mean and std.dev. for the whole image is computed over all pixels (not per pixel); in RGB images, standardization can be done e.g. for each colour channel individually; in other/non-image data sets, attribute-wise standardization should be applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 255.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.min(), img_array.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129.58246, 74.276726)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = img_array.mean()\n",
    "stddev = img_array.std()\n",
    "mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2709845e-07, 0.9999997)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array = (img_array - mean) / stddev\n",
    "img_array.mean(), img_array.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.7445904, 1.6885173)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.min(), img_array.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating NN Models in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1) Fully Connected NN\n",
    "\n",
    "For a fully connected neural network, the x and y axis of an image do not play a role at all. All pixels are considered as a completely individual input to the neural network. Therefore the 2D image arrays have to be flattened to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 4000)\n"
     ]
    }
   ],
   "source": [
    "#  flatten images to vectors\n",
    "images_flat = img_array.reshape(img_array.shape[0],-1)\n",
    "print images_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "# find out input shape for NN, which is just a long vector (40x100 = 4000)\n",
    "input_shape = images_flat.shape[1]\n",
    "print input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model\n",
    "\n",
    "In Keras, one can choose between a Sequential model and a Graph model. Sequential models are the standard case. Graph models are for parallel networks and use the functional API (see Music/Speech tutorial).\n",
    "\n",
    "Here we create a sequential model with 2 fully connected (a.k.a. 'dense') layers containing 256 units each.\n",
    "\n",
    "The output unit is a Single sigmoid unit which can predict values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simple Fully-connected network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_dim=input_shape))\n",
    "\n",
    "model.add(Dense(256))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 256)           1024256     dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 256)           65792       dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             257         dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1,090,305\n",
      "Trainable params: 1,090,305\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss Function and Optimizer Strategy: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'binary_crossentropy'  # 'categorical_crossentropy' for multi-class problems\n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model\n",
    "# This creates the whole model structure in memory. \n",
    "# If you use GPU computation, here GPU compatible structures and code is generated.\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.5093 - acc: 0.7714     \n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.3085 - acc: 0.8524     \n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.2567 - acc: 0.8867     \n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1809 - acc: 0.9267     \n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1580 - acc: 0.9419     \n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1303 - acc: 0.9581     \n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1050 - acc: 0.9743     \n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0899 - acc: 0.9829     \n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0812 - acc: 0.9829     \n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0678 - acc: 0.9914     \n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0574 - acc: 0.9924     \n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0490 - acc: 0.9962     \n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0415 - acc: 0.9981     \n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0357 - acc: 0.9990     \n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0316 - acc: 0.9981     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8b6ded4750>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "model.fit(images_flat, classes, batch_size=32, nb_epoch=epochs) #, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 896/1050 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify Accuracy on Train set\n",
    "predictions = model.predict_classes(images_flat)\n",
    "accuracy_score(classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% Accuracy - perfect, no?\n",
    "\n",
    "This is the accuracy on the training set. A (large, especially fully connected network with sufficient number of units) can easily learn the entire training set (especially a small one like here).\n",
    "\n",
    "This very likely leads to <b>overfitting</b>. That's why we test on an independent test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 170 files\n"
     ]
    }
   ],
   "source": [
    "path = 'data/CarData/TestImages'\n",
    "files = glob.glob(os.path.join(path, '*.pgm'))\n",
    "print \"Found\", len(files), \"files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from image_preprocessing import resize_and_crop\n",
    "\n",
    "test_images = []\n",
    "\n",
    "for filename in files:\n",
    "    with Image.open(filename) as img:\n",
    "        img_resized = resize_and_crop(img,target_width=100,target_height=40)\n",
    "        test_images.append(np.array(img_resized))\n",
    "        #print img.size, img_resized.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAoCAAAAAAtEwCfAAAL3UlEQVR4nAXBW4ydV3UA4LXW3vu/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\nL34ckgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x40 at 0x7F8B6DFD8D50>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=4\n",
    "Image.fromarray(test_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make 1 big array again from list\n",
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Test Set\n",
    "\n",
    "The test data has to be standardized <b>in the same way</b> as the training data for compatibility with the model! That means, we take the mean and standard deviation of the <i>training data</i> to transform also the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NO! we take the same mean and stddev from the training data above!\n",
    "#mean = test_images.mean()\n",
    "#stddev = test_images.std()\n",
    "#print mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121.71342647058823, 72.05232875177191)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.mean(), test_images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_images = (test_images - mean) / stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Images for Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 4000)\n"
     ]
    }
   ],
   "source": [
    "test_images_flat = test_images.reshape(test_images.shape[0],-1)\n",
    "print test_images_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/170 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_images_flat)\n",
    "# show 30 first predictions\n",
    "test_pred[0:30,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Groundtruth:\n",
    "# this TEST SET contains ONLY CARS on images! \n",
    "# Thus all the test classes are 1\n",
    "test_classes = [1] * len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's count the number of ones ...\n",
    "test_pred.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ALL our test classes are 1, counting the number of 1's and dividing by number of files gives us the Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45882352941176469"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.sum() / 170.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real way to do it is to compare the predictions (test_pred) with the ground truth (test_classes) and sum up the correct ones.\n",
    "This is exactly what the scikit-learn function <i>accuracy_score</i> does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45882352941176469"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(test_classes, test_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the Test Set is rather low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which groups neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Our input to the CNN is the standardized version of the original image array.\n",
    "\n",
    "#### Adding the channel\n",
    "\n",
    "For CNNs, we need to add a dimension for the color channel to the data. RGB images typically have an 3rd dimension with the color. \n",
    "<b>For greyscale images we need to add an extra dimension for compatibility with the CNN implementation.</b>\n",
    "\n",
    "In Theano, traditionally the color channel was the <b>first</b> dimension in the image shape. \n",
    "In Tensorflow, the color channel is the <b>last</b> dimension in the image shape. \n",
    "\n",
    "This can be configured now in ~/.keras/keras.json: \"image_dim_ordering\": \"th\" or \"tf\" with \"tf\" (Tensorflow) being the default image ordering even though you use Theano. Depending on this, use one of the code lines below.\n",
    "\n",
    "For greyscale images, we add the number 1 as the depth of the additional dimension of the input shape (for RGB color images, the number of channels is 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_channels = 1 # for grey-scale, 3 for RGB, but usually already present in the data\n",
    "\n",
    "if keras.backend.image_dim_ordering() == 'th':\n",
    "    # Theano ordering (~/.keras/keras.json: \"image_dim_ordering\": \"th\")\n",
    "    train_img = img_array.reshape(img_array.shape[0], n_channels, img_array.shape[1], img_array.shape[2])\n",
    "    test_img = test_images.reshape(test_images.shape[0], n_channels, test_images.shape[1], test_images.shape[2])\n",
    "else:\n",
    "    # Tensorflow ordering (~/.keras/keras.json: \"image_dim_ordering\": \"tf\")\n",
    "    train_img = img_array.reshape(img_array.shape[0], img_array.shape[1], img_array.shape[2], n_channels)\n",
    "    test_img = test_images.reshape(test_images.shape[0], test_images.shape[1], test_images.shape[2], n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 40, 100)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 1, 40, 100)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40, 100)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of images)\n",
    "    \n",
    "input_shape = train_img.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN model\n",
    "\n",
    "You may try to change the following to see the impact on the result:\n",
    "* number of filters\n",
    "* filter kernel size (e.g. 3 x 3, 5 x 5, ...)\n",
    "* adding/not adding Batch Normalization\n",
    "* adding/not adding ReLU Activation\n",
    "* adding/not adding Max Pooling\n",
    "* changing Pooling size (e.g. 1 x 2, 2 x 2, 2 x 1, or more)\n",
    "* adding/changing/removing Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createMyModel():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    n_filters = 16\n",
    "    # this applies n_filters convolution filters of size 5x5 resp. 3x3 each in the 2 layers below\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(Convolution2D(n_filters, 3, 3, border_mode='valid', input_shape=input_shape))\n",
    "    # input shape: 100x100 images with 3 channels -> input_shape should be (3, 100, 100) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))  # ReLu activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))) # reducing image resolution by half\n",
    "    model.add(Dropout(0.3))  # random \"deletion\" of %-portion of units in each batch\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Convolution2D(n_filters, 3, 3))  # input_shape is only needed in 1st layer\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten()) # Note: Keras does automatic shape inference.\n",
    "    \n",
    "    # Full Layer\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 16, 38, 98)    160         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 16, 38, 98)    392         convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 16, 38, 98)    0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 16, 19, 49)    0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 16, 19, 49)    0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 16, 17, 47)    2320        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 16, 17, 47)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 16, 17, 47)    0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 12784)         0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 256)           3272960     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 256)           0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 256)           0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             257         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3,276,089\n",
      "Trainable params: 3,275,893\n",
      "Non-trainable params: 196\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = createMyModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you get OverflowError: Range exceeds valid bounds in the above box, check the correct Theano vs. Tensorflow ordering in the box before and your keras.json configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "loss = 'binary_crossentropy' \n",
    "optimizer = 'sgd' \n",
    "#optimizer = SGD(lr=0.001)  # possibility to adapt the learn rate\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.4806 - acc: 0.8295     \n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1493 - acc: 0.9524     \n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1270 - acc: 0.9495     \n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0847 - acc: 0.9648     \n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0675 - acc: 0.9762     \n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0612 - acc: 0.9790     \n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0476 - acc: 0.9819     \n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0564 - acc: 0.9800     \n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0383 - acc: 0.9867     \n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0386 - acc: 0.9895     \n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0413 - acc: 0.9829     \n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0252 - acc: 0.9905     \n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0260 - acc: 0.9905     \n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0279 - acc: 0.9895     \n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0133 - acc: 0.9962     \n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you get ValueError: GpuDnnConv images and kernel must have the same stack size, best upgrade Keras to 1.2.0 as we experienced this problem when using BatchNormalization in Keras 1.1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Again, our Accuracy rises quickly to almost 100%, with the CNN now even faster than with the Fully Connected Network.\n",
    "But is our model really good at predicting unseen data?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Validation Data\n",
    "\n",
    "We split off 10 % of the training data and use it as independend validation set to verify how good we are\n",
    "on an independent data (not used for training) in each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945 samples, validate on 105 samples\n",
      "Epoch 1/15\n",
      "945/945 [==============================] - 0s - loss: 0.5864 - acc: 0.7841 - val_loss: 0.4166 - val_acc: 0.7905\n",
      "Epoch 2/15\n",
      "945/945 [==============================] - 0s - loss: 0.1894 - acc: 0.9259 - val_loss: 0.2158 - val_acc: 0.9619\n",
      "Epoch 3/15\n",
      "945/945 [==============================] - 0s - loss: 0.1054 - acc: 0.9630 - val_loss: 0.1644 - val_acc: 0.9429\n",
      "Epoch 4/15\n",
      "945/945 [==============================] - 0s - loss: 0.0873 - acc: 0.9683 - val_loss: 0.1035 - val_acc: 0.9714\n",
      "Epoch 5/15\n",
      "945/945 [==============================] - 0s - loss: 0.1006 - acc: 0.9577 - val_loss: 0.0904 - val_acc: 0.9714\n",
      "Epoch 6/15\n",
      "945/945 [==============================] - 0s - loss: 0.0784 - acc: 0.9746 - val_loss: 0.0726 - val_acc: 1.0000\n",
      "Epoch 7/15\n",
      "945/945 [==============================] - 0s - loss: 0.0551 - acc: 0.9841 - val_loss: 0.0612 - val_acc: 1.0000\n",
      "Epoch 8/15\n",
      "945/945 [==============================] - 0s - loss: 0.0552 - acc: 0.9841 - val_loss: 0.0757 - val_acc: 0.9905\n",
      "Epoch 9/15\n",
      "945/945 [==============================] - 0s - loss: 0.0416 - acc: 0.9905 - val_loss: 0.0440 - val_acc: 1.0000\n",
      "Epoch 10/15\n",
      "945/945 [==============================] - 0s - loss: 0.0367 - acc: 0.9873 - val_loss: 0.0362 - val_acc: 1.0000\n",
      "Epoch 11/15\n",
      "945/945 [==============================] - 0s - loss: 0.0336 - acc: 0.9884 - val_loss: 0.0363 - val_acc: 1.0000\n",
      "Epoch 12/15\n",
      "945/945 [==============================] - 0s - loss: 0.0308 - acc: 0.9905 - val_loss: 0.0398 - val_acc: 0.9905\n",
      "Epoch 13/15\n",
      "945/945 [==============================] - 0s - loss: 0.0244 - acc: 0.9915 - val_loss: 0.0284 - val_acc: 1.0000\n",
      "Epoch 14/15\n",
      "945/945 [==============================] - 0s - loss: 0.0176 - acc: 0.9937 - val_loss: 0.0313 - val_acc: 1.0000\n",
      "Epoch 15/15\n",
      "945/945 [==============================] - 0s - loss: 0.0163 - acc: 0.9968 - val_loss: 0.0276 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# recreate and recompile the model (otherwise we continue learning)\n",
    "model = createMyModel()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# train with showing accuracy on split off validation data\n",
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs, validation_split=0.1) # portion of val. data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the split-off validation data are quite high (usually similar, but not as high as on the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Test Set as Validation Set\n",
    "\n",
    "<b>Note: This usually is not recommended as during experimentation you will overfit also to the test data.</b>\n",
    "\n",
    "We show it here only for demonstration purposes to see how (bad) the validation accuracy is on our independet test data. The recommended way is to have a separate training, validation and test set (i.e. 3 splits or separate data sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1050 samples, validate on 170 samples\n",
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.3967 - acc: 0.8200 - val_loss: 0.8703 - val_acc: 0.3588\n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1710 - acc: 0.9429 - val_loss: 1.7188 - val_acc: 0.1118\n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1010 - acc: 0.9619 - val_loss: 2.3172 - val_acc: 0.0647\n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0838 - acc: 0.9695 - val_loss: 2.7548 - val_acc: 0.0706\n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0445 - acc: 0.9857 - val_loss: 3.8368 - val_acc: 0.0471\n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0477 - acc: 0.9819 - val_loss: 4.3550 - val_acc: 0.0588\n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0308 - acc: 0.9914 - val_loss: 3.2424 - val_acc: 0.1000\n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0526 - acc: 0.9781 - val_loss: 4.1435 - val_acc: 0.0824\n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0233 - acc: 0.9933 - val_loss: 3.5630 - val_acc: 0.1412\n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0177 - acc: 0.9971 - val_loss: 4.3540 - val_acc: 0.1118\n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0172 - acc: 0.9952 - val_loss: 6.0730 - val_acc: 0.0647\n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0188 - acc: 0.9943 - val_loss: 7.1931 - val_acc: 0.0529\n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0172 - acc: 0.9933 - val_loss: 6.9245 - val_acc: 0.0588\n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0112 - acc: 0.9990 - val_loss: 8.6260 - val_acc: 0.0471\n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0169 - acc: 0.9924 - val_loss: 6.7587 - val_acc: 0.0824\n"
     ]
    }
   ],
   "source": [
    "# recreate and recompile the model (otherwise we continue learning)\n",
    "model = createMyModel()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# show result on Test Data while training \n",
    "# we use test data as validation data to see direct results (usually NOT RECOMMENDED due to overfitting to the problem!)\n",
    "validation_data = (test_img, test_classes)\n",
    "\n",
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.81999999965940207,\n",
       "  0.94285714217594696,\n",
       "  0.96190476122356594,\n",
       "  0.96952380952380957,\n",
       "  0.98571428503308978,\n",
       "  0.98190476122356596,\n",
       "  0.99142857142857144,\n",
       "  0.97809523809523813,\n",
       "  0.99333333265213741,\n",
       "  0.99714285646166123,\n",
       "  0.99523809523809526,\n",
       "  0.99428571428571433,\n",
       "  0.99333333333333329,\n",
       "  0.99904761904761907,\n",
       "  0.99238095238095236],\n",
       " 'loss': [0.39672894330251784,\n",
       "  0.17101604864710854,\n",
       "  0.10101084103186925,\n",
       "  0.083790038146433377,\n",
       "  0.044469028853234789,\n",
       "  0.047731710636899585,\n",
       "  0.030785161579648654,\n",
       "  0.052611949464217538,\n",
       "  0.023272062653586979,\n",
       "  0.017700184312249932,\n",
       "  0.017168679502570915,\n",
       "  0.018818419288311686,\n",
       "  0.017184943739945689,\n",
       "  0.011223928680022558,\n",
       "  0.016917803221870035],\n",
       " 'val_acc': [0.35882352976238024,\n",
       "  0.11176470597000683,\n",
       "  0.064705882352941183,\n",
       "  0.07058823538177153,\n",
       "  0.047058823529411764,\n",
       "  0.058823529411764705,\n",
       "  0.10000000008765389,\n",
       "  0.082352941264124471,\n",
       "  0.14117647067588918,\n",
       "  0.11176470597000683,\n",
       "  0.064705882440595067,\n",
       "  0.052941176470588235,\n",
       "  0.058823529411764705,\n",
       "  0.047058823529411764,\n",
       "  0.082352941264124471],\n",
       " 'val_loss': [0.87029091470381792,\n",
       "  1.7188197444466984,\n",
       "  2.3172178380629598,\n",
       "  2.7547631067388196,\n",
       "  3.8367569979499367,\n",
       "  4.3550494081833779,\n",
       "  3.2423735927132999,\n",
       "  4.1434995314654186,\n",
       "  3.5630204873926501,\n",
       "  4.3540157149819763,\n",
       "  6.0729975700378418,\n",
       "  7.193082394319422,\n",
       "  6.9244857675888962,\n",
       "  8.6259993048275216,\n",
       "  6.7586808260749365]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>On the test set we perform really bad: less than 10% Accuracy!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_img)\n",
    "# show 35 first predictions\n",
    "test_pred[0:35,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/170 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.082352941176470587"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(test_img)\n",
    "accuracy_score(test_classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: We are completely overfitting!\n",
    "While we achieve nearly 100% on the Training Set, the generalization to the Test Set is really bad, with an Accuracy of about only 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Augmentation\n",
    "\n",
    "Increase the training set by adding more images: Rotate, shift, flip and scale the original images to generate additional examples that will help the Neural Network to generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ImageDataGenerator needs the classes as Numpy array instead of normal list\n",
    "classes_array = np.array(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) # enforce repeatable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.7123 - acc: 0.6010 - val_loss: 0.8059 - val_acc: 0.2118\n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.5517 - acc: 0.7010 - val_loss: 0.8343 - val_acc: 0.3118\n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.5087 - acc: 0.7524 - val_loss: 1.0815 - val_acc: 0.1118\n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.4209 - acc: 0.8029 - val_loss: 0.9153 - val_acc: 0.3765\n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.4048 - acc: 0.8162 - val_loss: 1.1389 - val_acc: 0.2647\n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.3704 - acc: 0.8438 - val_loss: 0.8409 - val_acc: 0.4824\n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.3726 - acc: 0.8362 - val_loss: 0.6340 - val_acc: 0.6412\n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.3397 - acc: 0.8581 - val_loss: 0.9270 - val_acc: 0.4471\n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.3258 - acc: 0.8619 - val_loss: 0.7628 - val_acc: 0.5647\n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.3024 - acc: 0.8743 - val_loss: 0.8554 - val_acc: 0.5294\n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.2881 - acc: 0.8810 - val_loss: 1.1256 - val_acc: 0.4294\n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.2821 - acc: 0.8895 - val_loss: 1.3891 - val_acc: 0.3647\n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.2297 - acc: 0.9067 - val_loss: 1.4574 - val_acc: 0.3765\n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.2592 - acc: 0.9029 - val_loss: 0.6697 - val_acc: 0.6824\n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 1s - loss: 0.2605 - acc: 0.8857 - val_loss: 0.7376 - val_acc: 0.6588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8b42576bd0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recreate and recompile the model (otherwise we continue learning)\n",
    "model = createMyModel()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(train_img, classes_array, batch_size=16),\n",
    "                    samples_per_epoch=len(train_img), nb_epoch=epochs,\n",
    "                    validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set (with Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/170 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_img)\n",
    "test_pred[0:35,0] # show 35 first predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/170 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6705882352941176"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(test_img)\n",
    "accuracy_score(test_classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Conclusion: With Data Augmentation, the model has more diverse training examples and generalizes much better. The Accuracy on the Test Set increases from less than 10% to more than 55% up to 75%!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
