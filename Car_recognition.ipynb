{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Recognition with Deep Learning\n",
    "\n",
    "This tutorial shows how Neural Networks are used to recognize cars on images vs. images with no cars on them (binary classification).\n",
    "\n",
    "The data set used is the [UIUC Image Database for Car Detection](http://cogcomp.cs.illinois.edu/Data/Car/) containing:\n",
    "* 1050 training images (550 car and 500 non-car images)\n",
    "* 170 test images, containing 200 cars at roughly the same scale as in the training images \n",
    "(we do not use the multi-scale test images, containing 139 cars at various scales, here)\n",
    "\n",
    "This tutorial contains:\n",
    "* Image Loading and Preprocessing\n",
    "* Standardization of Data\n",
    "* Fully Connected Neural Networks\n",
    "* Convolutional Neural Networks\n",
    "* Batch Normalization\n",
    "* ReLU Activation\n",
    "* Dropout\n",
    "* Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "device = 'gpu'\n",
    "os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=' + device + ',floatX=float32'\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from theano import config\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "#from keras.optimizers import SGD, RMSprop, Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed?\n",
    "import keras\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import pandas as pd # Pandas for easier Data handling in preparation\n",
    "\n",
    "from theano import function as tfunction\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import Callback, History, EarlyStopping, ModelCheckpoint # BaseLogger,\n",
    "\n",
    "import json\n",
    "#import cPickle # for saving scaler and labelencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Images from Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1050 files\n"
     ]
    }
   ],
   "source": [
    "path = 'data/CarData/TrainImages'\n",
    "files = glob.glob(os.path.join(path, '*.pgm'))\n",
    "print \"Found\", len(files), \"files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "image_names = []\n",
    "\n",
    "for filename in files:\n",
    "    image_names.append(os.path.basename(filename))\n",
    "    with Image.open(filename) as img:\n",
    "        images.append(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos-315.pgm\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAoCAAAAAAtEwCfAAAMNklEQVR4nIVXW4yc91X//S/f/Zv7\nzM7srr32ru34ltqOc2muiFY0jQi0AVoqofKC1MA78IKQAAmeeCkgtUhIlagQCgqgEJrQQkkb13WT\nrI2xnV1nfVnv2rs7OzM7szPffPfvf+HBTcML4jwc6ehI5+ic3zk6v0POaGWVhNLF3NOrwdKZY/R7\nbxuvPO9NEVy7vJVkYZ55vudaitF4DCpzk0lXZlEhTKUAJAyAzBUg2l95QW4dOLrxD4/+Ev/WXwBg\nBLYlwlnwSRodak+LJN+6MDuza+yIH105e7YxtVn/h9d9Cp+OU8MgalA+4IipI3arpWIUTs0FPhkA\nABgHuDWVQO2k7N3ibeLfeLHjAIDkJoQkDnjlQJnGSgEG1XkcLi8vzOpLpdn87ddKxx6YgKoalmvb\nzurwqYPv1B17LLO8gANNDQAAf6gVYHhyMk4jpccFI2AAAKVAQ8XdpZOjD7TWCkwkwzx0OBh33av/\nuPwUBQDluH6pZD2RRJ9pbq3UXUylQsmVkroCABQAABSQu6Vxwa1MSTke4mfChzmXk15MfmqnMsRc\n0D/38tLNv7p2JkhsBUCJzGTp+fPBI87Ba4khJCiQgxaMAUCmAMChSNTa4f0clMDEZPeTJFDgQfrA\n9gWBgiEyJIcXYvzXn3w5CPKpLyMHlA5ZJTGNfzcO39qJGtOYMg5wNcisFoeiVMYcAq4JS95n41wx\nKpJETT7JISoZ343E7KEsK7hDlZJG1nXmu1fvd45Z+35ntKOitCK2mF9699S2ZEGOacsaK89IjCP5\n2DC4iAUoNcEoICYDQak2GSJIgAAUAFAFh8cny+xgqxVed4CizIOgF88cOvG5tQtynjGvtGDcT3jd\n9G52fuPSX4ZuizRr3ftxpRGsNTpqyFTBLcccJVXflJPmST8qf+7erixXAIBzADRI+F+DdS+brU7d\nyAEgzxnjdGlpqs48vRm52dkjNWIncBgKQhpvKjMjzUY3g7wzetwYOLUp2qZhGOHEpxinlcW5Emvt\n/ctOJAEo01cUis/wReDQ05px5tUBAIQwQGQdXVuoXb1/4z/pZx9bmNu7kKk38HuNvSG6DzsdhrjC\nG6JsiRSEkqrHmN7jS7U59CuPvLsKX4FSBaUgOXgLoGYOThE8nEXOKRvsjUfjYmVtr2NM//TJ12Q6\nDc0XzQONvxkoGUhoVGj3Ut/0hnFvoplTYjOeprLTOFkzhdJiCoQAEE40JJMgAxBoodTHMCmloNLp\njqg3ML7Wnj3hmwQgRGug4IT8DE9VDK+k9rgwFagRCZPGk1q7ZouBR9UwEUISHe9/JIimmgyQKziA\nAFEKuWmKtMjqbhBoPWNnviUHN+69v1cAedHuccZF/qlXF3tXxl+YJVCX//z4C/VZl2IkPahMZlEQ\ncUoX5yRTOTjSHKBZTH4nW7BMw7crhnIJXIKCctegUEVUbWF3+dvd8tJc3Vm5U5Qf/bm6L9e/e++X\nj402J0cXzpeBYvDNi4uvnOLaUkXJRtzbipspO9QWrEgLZlNhcCZi8nJmgVm+d8zSe4I+ZStBTYtR\n0yoZ0Si9tTwzU0SJ07q5mpcfe9at1rs9OedN96NGkVVnWiYplr9Re7xxrDxKuTfYM1zCxn7NLLJy\nneYwU84YAbkrg0K5FAYw1bAICEHC7SpN3x+esEwepUSqrHd7Na93uHnw+PZe/Ui56G2mgTfbrDXa\nHOrO97/7lTO97v1+tvjIAk0zceFiFvzWr8zGkWcrqZXJy5LkMhfKNVGhCDUcqHmVRtPYOD7vRKM+\naVaqk10A2ixtJ8mUlwPHrYU7TtnSkcxnLP7I0S+sLk9SjLVVo2F4MBdbW/B1PE2lgFIx5yKvkWIq\nRF6gACEajCpDJkGSLxpxnmuHUZ1JgwOyeaY6UrMtq2TYVepSMVg3l4xdVsz7i3Pr23u53TpQURGK\n4sDhRodu9kcxVwb3shbZK6guhJAWCqFlwQ1PxbuuX+fQyc6Hav6IW4QqvLJ5V/mnFt4fHJ9t5yjX\nunv7ljW+cfrJTNYbKT9YZUQXIh5Od2SbYry2XQ0cvrGnTmLC4ufIvtLFNAnDssnAqGra8s7ff+/P\nXqIrb344Za7AoYp5dP7GH9q2L/yOErnm55qJN8Jx8IZtI1C+k6cOrzZtTokcvj44J6Pr/x2VX8C9\n4X76GEZZtEA2c1DkcXqWUxAkUtz79hsHvvzoXnx7xqKlaVot05ly9pHmlqrP6zwBO2EqxZkmKNTm\nTeGXhMGUT2PZu+w/WxmNosP08hXn/PRq32RBg4rIsEkAEwCoEHHSH9m+GzzY+OdPf97/lEEpFFAI\nyhQIAE2IZhRKSVls92/fLGaeY3omCTmnmlsiGN0rzdZcdFVJ7WxEcmXULsez++PYL5OU5qnoT6aE\nKi2EybgKhsOLg7LlLXl0RkUP9ptnXbHOoh3qNBSzoXLbLTlUzdPAjBJQAwaHYiggK1SbLAnibHtt\nVAkSRVWWZbAtsjJGOlRZel86jsXXEj5nFQpWDek4poXh1uX69rj0sppucABKaf+gRynRo/cqZzlV\nsBk4RyxN5vAhLTkW2M5ba5Vzt1jSTWGwmBqMvDaWQMnAPe1Q24qnk6jd7NW9/TwDkREKnaRm4p3Q\nIgdzaB6BW6KIYtREs65MWwgKg5BYetSzp6pK18RptbstnR8Uk3V4S7xHbYP8ehrQCiuVVF6ULbAi\nzAwWWTyRgAKzAEIJYxKUwGKAFp5QVJl1s5yOU3ueJZFirgFUUs8iW0Z926klDrYehMGtHeq0yZg5\nnDwtVmMwy3jWk0TDAAFgaRCpATDz4YkyOACYHIDwACirlu2mFbtki31hmZzZNI+i6dzRyjQrX/9g\nN2aV83O0f3XL9byyY/B7qePQRBW7tlUyjIeUDB9zpP9LqI52rmUkIMwgzfNuLhTLFaHNp8gfXVly\ny4V6/EmH0qpiw1QV3OBpMm+micBWucQNS+r/JzxMqgCa3usPkgab0hjRfuN051Q5nEyn18tO7Rfp\npWO/ltukNlm9bh7KkyizOK/iqBNWh8lgMu+WDSHBAFAUHxMz0P9dmYJgAGXRR5KUGqpUDvu9YLD/\nmZ0+pCjduuA/VV51t2+cs/OVoF+Za3mouAbnpnzpeG8nwnfWY21UmEasiU50fZoDMOtE5RJCaADg\noAZVnpYSwb2yXZtbaM3R1Tc/VO79Hevxo3X3VNcqvkVfWhk2T3pfE881RLbEickoeX72GWwlDk9v\nrs80a8dUGineuNj7/c0NE+A8L5KcJJQBQKdMLZeZSgTh5o+bc+c/bdxdXzoh3ngnW2w3GpzRtu8W\nYx3ctZ+teT+ZyqEgTxgTbhj86Fclfybvbm0IcY02ulQd5tRapH9X8zMg7CrbsIuyzQGAJJZXCd9i\nvswzz5lN/vX5I500FmfD9zaHC3Nq13RVSIAIyao5O40EKF22mxbnfPuPZ195tBYNe0mYWciZ3qXq\nQuVAOJkAYAvZcFCQvH1CAhiGIs/dz8aTYR6uNcXpA6J8Ynz1dcGNYseZlByHQwDwDhbSiH+w71UZ\n7hiLDc/iX/M7Lbv7zn9U06lVkoNMtAgqcsNhAJQx5DMnPeHdXqEA9jOrsaTfc+plzzzVOne8UTOV\n/diJ/vc3MkS3T1ZsohRAC8ltwy1t360zzo1MAvzfvjgJHPbK89+8lecxXI8PYNf716VZUaBRjPaC\nI93hR5QBTbrmbReb8zWTFzcacceeznu3L9UOdQA37dUWZS4EpUoAOYIvuZFQKDkoopBfX3u5k/oz\ntUn/yFedS2/HrD7JozO//XQiQDSRk4/e2jHv+IchU2rQz7/YqFnJrc29u4+7nWjvWLZW6A/WbueO\nlYx6tX6lyrKJW1ey1F6f/LwvUzVNhdolZBBQPioqN76x+AfzdPs3LwCPdl4vEYpPVkMDeKgI9EOH\nVttvfudLXxyz5t7Ftx90u0742K96bxT2q6Ovn39p9cbcE7n93IF83a9s359pmdxsaNHWzi+8WOQ5\nr339n9acZ59HLC0OQX+6iw8/Ng5AAhKgACGllXfHl1+tbvz4b5cB5Ki22r/7k4t3vONPuM1KOpls\nB3p4otOPh5PrPTKI8yqHUpzoMDd9Q+gwUmlerZljihDEgsoBwgkFVM68MDIciIadrGwo+5H6Xlda\nDtGKloNleXPz9OkQM5Md7h/09TM1UCV4sT7+H4zxjCDt0TT+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x40 at 0x7F3B6D93DB10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=5\n",
    "print image_names[i]\n",
    "Image.fromarray(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[i].shape   # height x width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Color RGB images have an additional dimension of depth 3, e.g. (40, 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make 1 big array of list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 40, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list of many 40x100 images is made into 1 big array\n",
    "# config.floatX is from Theano configration to enforce float32 precision (needed for GPU computation)\n",
    "img_array = np.array(images, dtype=config.floatX)\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Groundtruth based on filenames:\n",
    "\n",
    "In this data set, images with cars start with \"pos-\" and images with no cars start with \"neg-\". We create a numeric list here containing 1 for car images and 0 for non-car images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = []\n",
    "for name in image_names:\n",
    "    if name.startswith('neg'):\n",
    "        classes.append(0)\n",
    "    else:\n",
    "        classes.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first 25 classes\n",
    "classes[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundtruth Statistics:\n",
      "Class 0 : 500\n",
      "Class 1 : 550\n"
     ]
    }
   ],
   "source": [
    "print \"Groundtruth Statistics:\"\n",
    "\n",
    "for v in set(classes):\n",
    "    print \"Class\", v, \":\", classes.count(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5238095238095238"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = 550 * 1.0 / len(classes)\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "<b>Always standardize</b> the data before feeding it into the Neural Network!\n",
    "\n",
    "Here we use <b>Zero-mean Unit-variance standardization</b> (flat, i.e. one mean and std.dev. for the whole image is computed over all pixels; in RGB images, standardization can be done e.g. for each colour channel individually; in other/non-image data sets, attribute-wise standardization should be applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.582 74.2767\n"
     ]
    }
   ],
   "source": [
    "mean = img_array.mean()\n",
    "stddev = img_array.std()\n",
    "print mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.27098e-07 1.0\n"
     ]
    }
   ],
   "source": [
    "img_array = (img_array - mean) / stddev\n",
    "print img_array.mean(), img_array.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.7445904, 1.6885173)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.min(), img_array.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating NN Models in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1) Fully Connected NN\n",
    "\n",
    "For a fully connected neural network, the x and y axis of an image do not play a role at all. All pixels are considered as a completely individual input to the neural network. Therefore the 2D image arrays have to be flattened to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 4000)\n"
     ]
    }
   ],
   "source": [
    "#  flatten images to vectors\n",
    "images_flat = img_array.reshape(img_array.shape[0],-1)\n",
    "print images_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "# find out input shape for NN, which is just a long vector (40x100 = 4000)\n",
    "input_shape = images_flat.shape[1]\n",
    "print input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model\n",
    "\n",
    "In Keras, one can choose between a Sequential model and a Graph model. Sequential models are the standard case. Graph models are for parallel networks.\n",
    "\n",
    "Here we create a sequential model with 2 fully connected (a.k.a. 'dense') layers containing 256 units each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simple Fully-connected network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_dim=input_shape))\n",
    "\n",
    "model.add(Dense(256))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 256)           1024256     dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 256)           65792       dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             257         dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1090305\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "# This creates the whole model structure in memory. \n",
    "# If you use GPU computation, here GPU compatible structures and code is generated.\n",
    "\n",
    "loss = 'binary_crossentropy' \n",
    "optimizer = 'sgd' \n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.5201 - acc: 0.7800     \n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.3156 - acc: 0.8476     \n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.2316 - acc: 0.8924     \n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1758 - acc: 0.9324     \n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1453 - acc: 0.9486     \n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.1175 - acc: 0.9686     \n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0998 - acc: 0.9724     \n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0786 - acc: 0.9857     \n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0712 - acc: 0.9895     \n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0587 - acc: 0.9924     \n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0468 - acc: 0.9981     \n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0413 - acc: 0.9981     \n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0337 - acc: 0.9990     \n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0290 - acc: 1.0000     \n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 0s - loss: 0.0259 - acc: 0.9990     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b6cb16f90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "model.fit(images_flat, classes, batch_size=32, nb_epoch=epochs) #, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error when checking : expected convolution2d_input_4 to have 4 dimensions, but got array with shape (1050, 4000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-d1d5a620c12e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# verify Accuracy on Train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_flat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict_classes\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         '''\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         x = standardize_input_data(x, self.input_names,\n\u001b[0;32m   1165\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m                                    check_batch_dim=False)\n\u001b[0m\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[0;32m     98\u001b[0m                                 \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                                 \u001b[1;34m' dimensions, but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                                 str(array.shape))\n\u001b[0m\u001b[0;32m    101\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_dim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Error when checking : expected convolution2d_input_4 to have 4 dimensions, but got array with shape (1050, 4000)"
     ]
    }
   ],
   "source": [
    "# verify Accuracy on Train set\n",
    "predictions = model.predict_classes(images_flat)\n",
    "accuracy_score(classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% Accuracy - perfect, no?\n",
    "\n",
    "This is the accuracy on the training set. A (large, especially fully connected network with sufficient number of units) can easily learn the entire training set (especially a small one like here).\n",
    "\n",
    "This very likely leads to <b>overfitting</b>. That's why we test on an independent test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 170 files\n"
     ]
    }
   ],
   "source": [
    "path = 'data/CarData/TestImages'\n",
    "files = glob.glob(os.path.join(path, '*.pgm'))\n",
    "print \"Found\", len(files), \"files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from image_preprocessing import resize_and_crop\n",
    "\n",
    "test_images = []\n",
    "\n",
    "for filename in files:\n",
    "    with Image.open(filename) as img:\n",
    "        img_resized = resize_and_crop(img,target_width=100,target_height=40)\n",
    "        test_images.append(np.array(img_resized))\n",
    "        #print img.size, img_resized.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAoCAAAAAAtEwCfAAAMG0lEQVR4nDXUyY+l51WA8XPO+77f\neL871K15cFdXz90e43Y6xHaIg5AVywpSLIZYrJAiNoglgg1ix4YFEgIURCwkpEiwwsSRZRxZDsKO\njcFtt93dbvdQrqqu8VbVrXvvN77TYRHxLzw/6cE/vNkZZTHJUZIOxh33sJlDm/h9aSYqqQs1zlUa\nAAAk7fLyvH7vzhzBjKpKnWPaDSdDVkEk9Ezhe8lN1xUnuRDk828//uX4h8c/On3q0/d/cKWbyZXp\nGxiGUtBDDwHvFGa4wuWeTXdr01uqxy4AlgDYTMYt0k2JdaysrcyRTktTF0FUqfEkmzT9OE+S8tB6\nB6jSg42idXe/1z7m+Cd/9cy2/PjShUuPvnUR/qacNnK8nQadzm4p2QVUfWfvBlHSghoAi4ZmE2QM\nDSVTnVHdHFZRWnhXgSlKdDrbP0jicS1VSCTEw4fLp4/9lNP1+MxsWEtX/vYz8uvFH4ffuIcYzBxc\nPjOMx0tH/fP/0xFlqhhrZAAQCQsgGcVEaSvr6m3wwIDeCwpLz6AjadimhBAnZW7E0fIRp/XGZ6d+\nb9Z6+Qd5j7T69PXT3Inuu0uqSmuFl68PHw1Up1YLuhi7yDPE2chYz9bI0B0WYUKgvEcgFU17Rw7T\n1TmPB8KUNTsMpfVeCA6mei9+H5qz8rkvZ52F1zo/DMY7r4vTj/7CrXxcbW77G/XZufM7JfhUVCRM\nrZPpwFtdiaroyNb01QujY8eYJAvT9Wgyo1eDqDo2OsgyX0Mox4/Nk0TTv+J+ea4HcozK0+cfvfLq\nncO9MplEV7+789PaZ+nWfvPxwZoqx13Qy+7YxhdmEXhuZ2nqC0Mno6+34bGb22qu0z15MHsm/MKb\ng5KSwmTkoRxYHvSTiI8Ox6J/VqA8ntwI+Z9HX96cn3uPmugszDi38p0Df35gj4qisguX9i6Gu/Hm\n4gygUIuDztWzn8SwejTI8sqJnEU1CebTU4cTuZQeOY6U93k6FYRe5RMt583dl6JC+rC2dnzu5aX9\nW0PfWbr44U/bq5hke+3FPU+lo1Z8beNdDMpP/RUgoTrZkJ9uZed+8vmpug5cPpHpfDxQprG+waLy\ngQhmyvh8+JWP9vJ50d976wcLoQy9S05O9fbXJ3/aX5rKP8GTennvwMlsQuE53uKVB+aorML+8ZiP\nEzc6zj5bf1nIp/9zeoeXB+6U91WHNk+GrdCe+KOSS8W91ZN3TdIc16bJr04tWQ0ysp3+4PPpwTDc\nmf3N0f5Of77ORSUXL92cX7j35uZzffVO9/kPafVy4HfV2qmTu2txs3q52Xhyb2vx1MPW6Vt1dLM5\nnqr0anJr3tuW4qSNw6FtfIDWGP+NLC/k2npnJvyLa39yPHhxIoL1/bML+b0WU3699EdDiG3RrLTa\nZ5K5RkZ+SGpGP3Z2Wq5/sDJ76YF5dMHdr5ejrcK0w0mQuQyNZ1IuOTOiLJPeZIfjx+v8QFIwhmpl\n+4/e/Ie/+9tJkt0vnJe76apnU93dDVqjbrg2Wt844xlMGPh8ZVXPTvmZ1WebVnTv1kx3/Y6u46VW\nHbTa2RNFVeSMSc9M4jZqhD09X9YizOWh2t/HNf1O8R9ziRUXtyaVp/xkwKELExotd6ww+oRsnSnf\nM6BM9szTj/jaxWx3raHnXtrdu3c737P9Xh9XrHUWKAm89QJJ+B0QhQsEyncmtxCL9uLmA++gyFfH\nDTlMOwQm1/3VyB7eFDJotaIkgHYV9y5fmFGHLCpdqrsfZadPJ53T16q7n+ebOVjfBgrQGgmO0LHU\n+YkdBCBqCeJKrGCY/a57gCLfc/1Ql/1zIVsR1EaVo+1d9LO9JBWIjzw9vvnp/JmRJaQ+bq1PvQAb\niS7X5dK388MvBiXjSZ4EDdqqyFuP4DC9YvBi19hQPi9RhHYse6/rpOxmP6Op+2Ln/eWovN/hfCWt\nFpYrylJAw1NXerf/aQgLmrXTYnDbXFjYef9otj009VR6qnWvXrQ/FxGkkXWuUh0RytRbDEcp4B3v\nkU1jt29GD7AW1+sn95vDmVaWFtBQGEZxOn8xKMahpNSO/Sf/+zvn91Q9ZmeVFGhTJ6wUpibcOWJ5\n2LuGVmipyGoKlQdgGhw/cYS/0N5b7ciAumseNs6uBfsyqiWz43RhuUvT47t1PWty55o6TXYFV9x4\nMk5QZW1Ex+JkDEEtE4gHH//9qweV0oLIIHIEmiR+/t6l83hDe+8chmwV+clx6kXsLMiN+2Ow58rW\nwuab97kO1BOdo0ZmDXmqWqPRvFRVWH82NV08PFo5qMJxO5sWUEcvXHsS2AELDsBK0aA6fjvqt+Us\nF0YAArPz1FlhtA5T+/Dh3Gi88/aF9uGdzZcu1sW7b3xTTzoTQJZcoTomgpZR1aFVdf+V9vXrG9gK\nN57MXrv5l7F2tQsQkBHRuhw3ZkB6L4QgdOzBOSRlIUIXBW+PO1tn1vpT13defOnMN/88thuPNw0A\nAwBATGzDfHK5Oj778+Xff7V68V9em9xZbDaaw9OwXUrtE3aAKJ3TUy+VeS2tk2C0dZ6ZAQVJ6SES\n2nx6bsEP3dZXC/3ur9vvL/34SHovCKUCYEQEj7NpEw6S5+Pb58fp0z/rtfR6+6+/1+1rJCTwAMzK\nfjLwx2UsW4yshfYAFhRAFArHEi7/2b+++8FCODmog/P6g4XdQX14c2dBV4aVADAGWU73xwfq7lMd\n3ofR2htnn+1H9LXf8tWU84I9sgDQFkcHlUtRagSgOCJvQAAgkWfWDN/6jf96Y9LZLT/aunFV4uO3\nH315IpMtt5wY5b2IhWPj55LXb/33vUvPJcs/fuS15cYLtekiW3uu6kicQNb2o42VtoZSegL2SA7I\nGUHIgMTs/dhdOu8Lm/772+OjFdF8+MorG1E1HhzGvnEQBNDrt/VsuR2Sr0fiYPJsfcMzbakoGR27\nSEIqc1NeuzBo356ZbxgL+6vCJKwVBIQIzgF66xEthYBNdf+TxTu9Lja94SHPlEdmISQpw6KnuNNE\nbdBVkHbySkkwABCmyOw9BMHu3lQ4uvXIgiFkYARGAI8I4ECwQ8EIDMDIiOxkOWyMVQ23hNgangtO\nkhAlM1bjaaFDGzdNUwXCxuhrYM9ESOiY/O5oBrpdW3mD19MUnIqqsDaxboFhOMGIe6yZxugzXxAB\nbZJgFzWMhycZYxQCA5KI2g64lsD5uCsFKLLCOsUCBddS4nv/liRXr4WBNvKjLAFPZpIlDnyIdR4y\now+wqqnwaVjliSUEUQTKiLpRSVUBKzAIGECrm3rNodeB0cpokRNLGAsJpiG79NTkq/nLqt5VCb7G\nnshWAkQIDbN0DtGU7KOQBRlPoioYAAWw49rLQAIDWgYAb7l/NQnYkUTnnVHKusBrL1FHkX/QdIIh\nrEgMAeUEiBzFuimyHiMWiICOkbQFBvSMXjGAtgK8B3A1MDAiAIBgrm7FobRA0mfK1IF3SjoBojnS\n7Xu3ePWF8K27s19jhz8CACZhisPuYo54EkpG0wA6BrSaABSA58oRkgvJWI8ASASAqdFBIaBaXVO2\n0KBkFAKJyCg0CFK6B7986myjKFAsYwBg8CqLpY8Y+4jAViAQSgbjAcEhYipqqyVyEDFQt5gIAJZh\nzosgjdtfWWkDofGRYM81kg4C47P049LOTzeGEP8RPAMKBALHgJYAuPQATgMTCgDUgIDWOScBWAIA\nAFoAAGamQKA2FPS6s+3GCUXgNQE7w0zlw4uhslqyk44dAjgAQPAAtUWATAB4pwFZSMaGAaCWiArZ\nMQA4Ig8ARABeAsRo8qHLszZVAbg4tABSTAJer8lUAjUKaYAAQAMAAgN4AwATCeCtQQBJv1Jm4QEd\ns3XEHPKEAUhJAF8DIGAIJ6nZjueUEFwxoK4Dp7KJC2wlHUvZYATgLQAAegARI0Kl//8DBqQZAwGF\nCFhZqVSN4IQChCZHAARmdiIta0ZTB0GWiZoAGQQdZN9aw7JmC1IqJGaZAACXgBBj47hD0rOrQUuS\nKcfMzhEjhq5EYVE5mWjjW5LRGSIliGF+UMtuVnzRmZqZZc+MIZ+8vjnTfmJlscWF7LAhIsPA0YxD\nZqsEgQyBXUSO8Aj7Ya2kUJ4hJavtOGsrYSlSsfPSt6AJCQgo15Ft40igF8KaMdNE/lpSJJjvhsL/\nHye3L2cfHM0uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x40 at 0x7F3B6CC62B10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=3\n",
    "Image.fromarray(test_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make 1 big array again from list\n",
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Test Set\n",
    "\n",
    "The test data has to be standardized <b>in the same way</b> as the training data for compatibility with the model! That means, we take the mean and standard deviation of the <i>training data</i> to transform also the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NO! we take the same mean and stddev from the training data above!\n",
    "#mean = test_images.mean()\n",
    "#stddev = test_images.std()\n",
    "#print mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121.71342647058823, 72.05232875177191)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.mean(), test_images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_images = (test_images - mean) / stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Images for Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 4000)\n"
     ]
    }
   ],
   "source": [
    "test_images_flat = test_images.reshape(test_images.shape[0],-1)\n",
    "print test_images_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/170 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_images_flat)\n",
    "# show 30 first predictions\n",
    "test_pred[0:30,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Groundtruth:\n",
    "# this TEST SET contains ONLY CARS on images! \n",
    "# Thus all the test classes are 1\n",
    "test_classes = [1] * len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's count the number of ones ...\n",
    "test_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(test_classes, test_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117647058823529"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "70.0/170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the Test Set is only 41.17%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which groups neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Our input to the CNN is the standardized version of the original image array.\n",
    "\n",
    "#### Adding the channel\n",
    "\n",
    "For CNNs, we need to add a dimension for the color channel to the data. RGB images typically have an 3rd dimension with the color. \n",
    "<b>For greyscale images we need to add an extra dimension for compatibility with the CNN implementation.</b>\n",
    "\n",
    "In Theano, traditionally the color channel was the <b>first</b> dimension in the image shape. \n",
    "In Tensorflow, the color channel is the <b>last</b> dimension in the image shape. \n",
    "\n",
    "This can be configured now in ~/.keras/keras.json: \"image_dim_ordering\": \"th\" or \"tf\" with \"tf\" (Tensorflow) being the default image ordering even though you use Theano. Depending on this, use one of the code lines below.\n",
    "\n",
    "For greyscale images, we add the number 1 as the depth of the additional dimension of the input shape (for RGB color images, the number of channels is 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_channels = 1 # for grey-scale, 3 for RGB, but usually already present in the data\n",
    "\n",
    "if keras.backend.image_dim_ordering() == 'th':\n",
    "    # Theano ordering (~/.keras/keras.json: \"image_dim_ordering\": \"th\")\n",
    "    train_img = img_array.reshape(img_array.shape[0], n_channels, img_array.shape[1], img_array.shape[2])\n",
    "    test_img = test_images.reshape(test_images.shape[0], n_channels, test_images.shape[1], test_images.shape[2])\n",
    "else:\n",
    "    # Tensorflow ordering (~/.keras/keras.json: \"image_dim_ordering\": \"tf\")\n",
    "    train_img = img_array.reshape(img_array.shape[0], img_array.shape[1], img_array.shape[2], n_channels)\n",
    "    test_img = test_images.reshape(test_images.shape[0], test_images.shape[1], test_images.shape[2], n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 40, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 1, 40, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40, 100)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of images)\n",
    "    \n",
    "input_shape = train_img.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "n_filters = 16\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "\n",
    "model.add(Convolution2D(n_filters, 5, 5, border_mode='valid', input_shape=input_shape))\n",
    "# input shape: 100x100 images with 3 channels -> input_shape should be (3, 100, 100) \n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.3)) \n",
    "\n",
    "# B)\n",
    "model.add(Convolution2D(n_filters, 3, 3))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten()) # Note: Keras does automatic shape inference.\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get OverflowError: Range exceeds valid bounds in the above box, check the correct Theano vs. Tensorflow ordering in the box before and your keras.json configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_7 (Convolution2D)  (None, 16, 36, 96)    416         convolution2d_input_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 16, 36, 96)    192         convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 16, 36, 96)    0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 16, 18, 48)    0           activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 16, 18, 48)    0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 16, 16, 46)    2320        dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 16, 16, 46)    0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 16, 16, 46)    0           activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 11776)         0           dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 256)           3014912     flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 256)           0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 256)           0           activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 1)             257         dropout_12[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 3018097\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "loss = 'binary_crossentropy' \n",
    "optimizer = 'sgd' #rmsprop'\n",
    "#optimizer = SGD(lr=0.001)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1050 samples, validate on 170 samples\n",
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.5532 - acc: 0.8000 - val_loss: 1.4532 - val_acc: 0.1059\n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1616 - acc: 0.9410 - val_loss: 2.2924 - val_acc: 0.0706\n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1158 - acc: 0.9610 - val_loss: 2.7668 - val_acc: 0.0941\n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0903 - acc: 0.9610 - val_loss: 3.1192 - val_acc: 0.1059\n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0712 - acc: 0.9743 - val_loss: 3.0693 - val_acc: 0.1294\n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0644 - acc: 0.9800 - val_loss: 4.0261 - val_acc: 0.0941\n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0474 - acc: 0.9781 - val_loss: 3.4460 - val_acc: 0.1647\n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0346 - acc: 0.9914 - val_loss: 4.9928 - val_acc: 0.0882\n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0433 - acc: 0.9848 - val_loss: 3.9525 - val_acc: 0.1471\n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0277 - acc: 0.9933 - val_loss: 4.5292 - val_acc: 0.1294\n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0270 - acc: 0.9952 - val_loss: 4.9917 - val_acc: 0.1118\n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0232 - acc: 0.9943 - val_loss: 4.8375 - val_acc: 0.1353\n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0198 - acc: 0.9952 - val_loss: 5.5056 - val_acc: 0.1176\n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0142 - acc: 0.9981 - val_loss: 6.1983 - val_acc: 0.0941\n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0119 - acc: 0.9981 - val_loss: 6.9596 - val_acc: 0.0824\n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Validation Data\n",
    "\n",
    "We split off 10 % of the training data and use it as independend validation set to verify how good we are\n",
    "on an independent data (not used for training) in each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945 samples, validate on 105 samples\n",
      "Epoch 1/15\n",
      "945/945 [==============================] - 5s - loss: 0.0698 - acc: 0.9788 - val_loss: 0.0683 - val_acc: 0.9810\n",
      "Epoch 2/15\n",
      "945/945 [==============================] - 5s - loss: 0.0489 - acc: 0.9841 - val_loss: 0.0451 - val_acc: 0.9905\n",
      "Epoch 3/15\n",
      "945/945 [==============================] - 5s - loss: 0.0314 - acc: 0.9905 - val_loss: 0.0319 - val_acc: 0.9905\n",
      "Epoch 4/15\n",
      "945/945 [==============================] - 5s - loss: 0.0284 - acc: 0.9905 - val_loss: 0.0340 - val_acc: 0.9905\n",
      "Epoch 5/15\n",
      "945/945 [==============================] - 5s - loss: 0.0227 - acc: 0.9947 - val_loss: 0.0282 - val_acc: 0.9905\n",
      "Epoch 6/15\n",
      "945/945 [==============================] - 5s - loss: 0.0173 - acc: 0.9947 - val_loss: 0.0324 - val_acc: 0.9905\n",
      "Epoch 7/15\n",
      "945/945 [==============================] - 5s - loss: 0.0107 - acc: 0.9989 - val_loss: 0.0236 - val_acc: 0.9905\n",
      "Epoch 8/15\n",
      "945/945 [==============================] - 5s - loss: 0.0083 - acc: 0.9989 - val_loss: 0.0201 - val_acc: 0.9905\n",
      "Epoch 9/15\n",
      "945/945 [==============================] - 5s - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0201 - val_acc: 0.9905\n",
      "Epoch 10/15\n",
      "945/945 [==============================] - 5s - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0194 - val_acc: 0.9905\n",
      "Epoch 11/15\n",
      "945/945 [==============================] - 5s - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0218 - val_acc: 0.9905\n",
      "Epoch 12/15\n",
      "945/945 [==============================] - 5s - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0205 - val_acc: 0.9905\n",
      "Epoch 13/15\n",
      "945/945 [==============================] - 5s - loss: 0.0055 - acc: 0.9989 - val_loss: 0.0212 - val_acc: 0.9905\n",
      "Epoch 14/15\n",
      "945/945 [==============================] - 5s - loss: 0.0058 - acc: 0.9989 - val_loss: 0.0173 - val_acc: 0.9905\n",
      "Epoch 15/15\n",
      "945/945 [==============================] - 5s - loss: 0.0055 - acc: 0.9989 - val_loss: 0.0332 - val_acc: 0.9810\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs, validation_split=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1050 samples, validate on 170 samples\n",
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0058 - acc: 0.9981 - val_loss: 6.4739 - val_acc: 0.1118\n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0071 - acc: 0.9981 - val_loss: 6.2653 - val_acc: 0.1176\n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0044 - acc: 1.0000 - val_loss: 6.3239 - val_acc: 0.1176\n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0040 - acc: 1.0000 - val_loss: 7.3631 - val_acc: 0.0941\n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0037 - acc: 1.0000 - val_loss: 6.8158 - val_acc: 0.1118\n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0047 - acc: 0.9990 - val_loss: 7.2102 - val_acc: 0.1059\n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0039 - acc: 0.9990 - val_loss: 7.0418 - val_acc: 0.1176\n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0036 - acc: 1.0000 - val_loss: 7.3534 - val_acc: 0.1118\n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0070 - acc: 0.9971 - val_loss: 6.8198 - val_acc: 0.1118\n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0041 - acc: 0.9981 - val_loss: 7.0478 - val_acc: 0.1118\n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0039 - acc: 0.9990 - val_loss: 6.9055 - val_acc: 0.1176\n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0018 - acc: 1.0000 - val_loss: 7.0813 - val_acc: 0.1118\n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0035 - acc: 0.9990 - val_loss: 7.4954 - val_acc: 0.1059\n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0014 - acc: 1.0000 - val_loss: 7.6377 - val_acc: 0.1059\n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.0016 - acc: 1.0000 - val_loss: 7.2672 - val_acc: 0.1176\n"
     ]
    }
   ],
   "source": [
    "# Show Result on Test Data while training \n",
    "# we use test data as validation data to see direct results (usually NOT RECOMMENDED due to overfitting to the problem!)\n",
    "validation_data = (test_img, test_classes)\n",
    "\n",
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_img)\n",
    "# show 35 first predictions\n",
    "test_pred[0:35,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.58823529411764708"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(test_img)\n",
    "accuracy_score(test_classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: We are completely overfitting!\n",
    "While we achieve nearly 100% on the Training Set, the generalization to the Test Set is really bad, with an Accuracy of about only 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Augmentation\n",
    "\n",
    "Increase the training set by adding more images: Rotate, shift, flip and scale the original images to generate additional examples that will help the Neural Network to generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    #zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "#datagen.fit(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ImageDataGenerator needs the classes as Numpy array instead of normal list\n",
    "classes_array = np.array(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1626 - acc: 0.9400 - val_loss: 1.4034 - val_acc: 0.4118\n",
      "Epoch 2/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1518 - acc: 0.9352 - val_loss: 1.2689 - val_acc: 0.4588\n",
      "Epoch 3/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1627 - acc: 0.9333 - val_loss: 1.4350 - val_acc: 0.4059\n",
      "Epoch 4/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1566 - acc: 0.9438 - val_loss: 1.1026 - val_acc: 0.5000\n",
      "Epoch 5/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1669 - acc: 0.9343 - val_loss: 1.1472 - val_acc: 0.4824\n",
      "Epoch 6/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1508 - acc: 0.9419 - val_loss: 1.0082 - val_acc: 0.5235\n",
      "Epoch 7/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1944 - acc: 0.9248 - val_loss: 1.1814 - val_acc: 0.4294\n",
      "Epoch 8/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1447 - acc: 0.9400 - val_loss: 1.4667 - val_acc: 0.4059\n",
      "Epoch 9/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1661 - acc: 0.9371 - val_loss: 1.5967 - val_acc: 0.3647\n",
      "Epoch 10/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1636 - acc: 0.9314 - val_loss: 0.9733 - val_acc: 0.5529\n",
      "Epoch 11/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1701 - acc: 0.9362 - val_loss: 1.0711 - val_acc: 0.5059\n",
      "Epoch 12/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1604 - acc: 0.9410 - val_loss: 0.8782 - val_acc: 0.5824\n",
      "Epoch 13/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1574 - acc: 0.9448 - val_loss: 1.7521 - val_acc: 0.3471\n",
      "Epoch 14/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1482 - acc: 0.9448 - val_loss: 1.4979 - val_acc: 0.3706\n",
      "Epoch 15/15\n",
      "1050/1050 [==============================] - 6s - loss: 0.1504 - acc: 0.9410 - val_loss: 1.5066 - val_acc: 0.3765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History object at 0x7f3b5a61f390>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(train_img, classes_array, batch_size=32),\n",
    "                    samples_per_epoch=len(train_img), nb_epoch=epochs,\n",
    "                    validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set (with Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_img)\n",
    "# show 35 first predictions\n",
    "test_pred[0:35,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44117647058823528"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(test_img)\n",
    "accuracy_score(test_classes, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
